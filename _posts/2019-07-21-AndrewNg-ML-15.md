---
layout:	post
title:	"Machine Learning Notes: Lecture 15"
description: "Andrew Ng Machine Learning Notes for Lecture 15: Anomaly Detection"
date:	2019-07-21
tags:	["Machine Learning", "Andrew Ng"]
---

# 15. Anomaly Detection
Suppose we have a dataset of size $$m$$ with $$n$$ features. The anomaly detection aims at find out whether a new data point is an anomaly.

## 15.1 Algorithm
Suppose the features are independent from each other, and each of them is normally distributed. So we have $$p(x_j; \mu_j, \sigma_j^2) = \frac{1}{\sqrt{2\pi}\sigma_j} e^{-\frac{(x_j - \mu_j)^2}{2 \sigma^2}}$$. So the algorithm is the following:
1. Choose features $$x_i$$ that you think might be indicative of anomalous examples.
2. Compute parameters $$\mu_j = \frac{1}{m} \sum_{i=1}^m x_j^{(i)}$$ and $$\sigma_j^2 = \frac{1}{m} \sum_{i=1}^m (x_j^{(i)} - \mu_j)^2$$.
3. Given new example $$x$$, compute $$p(x) = \Pi_{j=1}^n p(x_j; \mu_j, \sigma_j^2)$$.
4. If $$p(x) < \epsilon$$, then $$x$$ is an anomaly. Here, $$\epsilon$$ is a given number.

## 15.2 Evaluation
The evaluation of the anomaly detection algorithm is quite similar with that of a supervised learning algorithm. You have to have a labeled dataset which contains anomalous data, and divide it into two parts: cross-validation part and testing part (see Machine Learning Notes: Lecture 10). Use cross-validation dataset to select a best $$\epsilon$$ for your anomaly detection algorithm and use testing dataset to test it. The error matrics for testing can be *F score* (see Machine Learning Notes: Lecture 11).

## 15.3 Comparison with Supervised Learning
Anomaly Detection works well in the following situations:
* You have a very small number of positive examples (e.g. 0-20).
* You have a large number of negative examples.
* There are many types of anomalies which make it hard for machine to learn from them.
* Future anomalies may look nothing like any of training examples we have so far.

While supervised learning works well when:
* You have a large number of both positive and negative examples.
* Future positive examples are likely to be similar to some of our examples in the training set.

## 15.4 Feature Selection
Recall that one of our hypotheses is that the features are all normally distributed. When we plot our training data with a feature and only to find that the feature is not normally distributed, we have to apply some functions onto this feature, e.g. $$log(x)$$ or $$x^0.5$$.

The ideal effect of our $$p(x)$$ is that it can distinguish normal examples from anomalous examples well, i.e. $$p(x)$$ is large for normal examples and small for anomalous examples. If we find that there is an anomalous example whose $$p(x)$$ is quite large, we have to seek for a new feature according to this anomaly.

## 15.5 Multivariate Gaussian Distribution
Recall that the other of our hypotheses is that the features are independent from each other. When this hypothesis is broken, we have to use multivariate gaussian (normal) distribution.

In multivariant gaussian distribution, there are 2 parameters: $$\mu$$ and $$\Sigma$$. Vector $$\mu = \frac{1}{m} \sum_{i=1}^m x^{(i)}$$ denotes the average values of all features and $$\Sigma = \frac{1}{m} \sum_{i=1}^m (x^{(i)} - \mu)(x^{(i)} - \mu)^T$$ is the covariance matrix. Given these 2 parameters, we have:

$$p(x;\mu,\Sigma) = \frac{1}{(2\pi)^{\frac{n}{2}} \vert \Sigma \vert ^{\frac{1}{2}}} e^{-\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu)}$$

where $$n$$ is the number of features. It is easy to see that the $$\mu$$ controls the position of the peek of its plot, and $$\Sigma$$ controls its shape.

## 15.6 Comparison between Original and Multivariant Models
Actually, in multivariant model, if $$\Sigma$$ is a **diagonal matrix**, then it corresponds to the original model.

For the original model:
* You have to manually create features to capture anomalies where $$x_1,x_2$$ take unusual combinations of values.
* It is computationally cheaper. It scales better to large $$n$$.
* It works even when $$m$$ is small.

For the multivariant model:
* It automatically capture the corelations between features.
* It is computationally more expansive. Because it has to compute $$\Sigma^{-1}$$.
* $$m$$ has to be larger than $$n$$, or $$\Sigma$$ is non-invertible.
