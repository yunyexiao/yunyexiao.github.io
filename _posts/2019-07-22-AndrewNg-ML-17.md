---
layout:	post
title:	"Machine Learning Notes: Lecture 17"
description: "Andrew Ng Machine Learning Notes for Lecture 17: Large Scale Machine Learning"
date:	2019-07-22
tags:	["Machine Learning", "Andrew Ng"]
---

# 17. Large Scale Machine Learning
In practice, "it's not who has be best algorithm that wins. It's who has the most data." The size of the training set is always at the level of $$10^8$$ or even higher. The batch gradient descent is computationally too expensive.

## 17.1 Stochastic Gradient Descent
The computational cost in batch gradient descent mainly lies in that you have to compute a summation of size $$m$$ to make a single step forward. Stochastic gradient descent gives up the summation and make a single step forward according to a single row of data. The algorithm is as follows:
1. Randomly shuffle the training set.
2. Repeat the following steps:
    1. for $$i = 1,2,...,m$$, update $$\theta_j := \theta_j - \alpha (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$$ for each $$j=0,1,...,n$$.

Usually, the outer loop will only run 1-10 times. As you may have considered, though this algorithm can run much faster than batch gradient descent, it may not converge. Actually, it will end up moving in a small region around the global minimum. But as long as it can reach to a point near the global minimum, it can work quite well.

To judge whether it reaches the global minimum, we have to plot the cost function $$cost(\theta, x^{(i)}, y^{(i)}) = \frac{1}{2} (h_\theta(x^{(i)}) - y^{(i)})^2$$ every, say 1000 or 5000, times. We have to plot it averaged over the past 1000 or 5000 times.

If we change the learning rate from a constant to a variance of iteration number $$\alpha = \frac{const_1}{iterationNumber + const_2}$$, then the algorithm may converge eventually. But in this way, you introduce a new parameter and add to the difficulty of your learning algorithm. The learning rate $$\alpha$$ is typically held constant.

## 17.2 Mini-batch Gradient Descent
The idea of mini-batch gradient descent is simple: reduce the size of summation in each update step. But different from stochastic gradient descent which reduces it to only one, this algorithm reduces it to $$b$$ where the value of $$b$$ is up to the user.

Split the training set into several blocks of size $$b$$, and use batch gradient descent on each block consecutively in each outer iteration.

## 17.3 Map Reduce and Data Parallelism
You can use map-reduce method to speed up your learning algorithm. (For more information, see [Wikipedia MapReduce](https://en.wikipedia.org/wiki/MapReduce))
