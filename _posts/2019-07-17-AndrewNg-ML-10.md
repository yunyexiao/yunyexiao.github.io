---
layout:	post
title:	"Machine Learning Notes: Lecture 10"
description: "Andrew Ng Machine Learning Notes for Lecture 10: Advice for Applying Machine Learning"
date:	2019-07-17
tags:	["Machine Learning", "Andrew Ng"]
---

# 10. Advice for Applying Machine Learning

## 10.1 When Prediction Error is Large
Suppose you have trained your model with a training set but only to find that when new dataset occurs, what your model predicts is a huge error. You can try the following things:

* Get more training examples. This helps fix high variance problems.
* Try smaller set of features. This helps fix high variance problems.
* Try getting additional features. This helps fix high bias problems.
* Try adding polynomial features e.g. $$x_1^2, x_2^2, x_1x_2$$. This helps fix high bias problems.
* Try decreasing regularization parameter $$\lambda$$. This helps fix high bias problems.
* Try increasing regularization parameter $$\lambda$$. This helps fix high variance problems.

## 10.2 Evaluating a Hypothesis
Obviously, we cannot evaluate our hypothesis using our training set, because if your model overfits, the performance will be quite good but it is not generalizable. We have to use another set of data to test our hypothesis.

Divide your dataset into two parts: training set (maybe 70%) and testing set (the rest), and as the name suggests, use the training set for training purposes and the testing set for testing (evaluating) purposes. The computation of the test error of a hypothesis is seemingly identical to that of the training error. E.g. for linear regression, the test error can be computed by:

$$J_{test}(\theta) = \frac{1}{2m_{test}} \sum_{i=1}^{m_{test}} (h_\theta(x_{test}^{(i)}) - y_{test}^{(i)})^2$$

For logistic regression, there is another way to compute the test error:

$$Test\ error = \frac{1}{m_{test}} \sum_{i=1}^{m_test} err(h_\theta(x_{test}^{(i)}), y_{test}^{(i)})$$

, where $$
err(h_\theta(x), y) = \left\{
\begin{array}{ll}
1, if\ h_\theta(x) \geq 0.5\ and\ y = 0\ or\ h_\theta(x) < 0.5\ and\ y = 1 \\
0, otherwise
\end{array}\right.
$$. This kind of test error is called *Misclassification Error*.

## 10.3 Model Selection
The problem occurs when selecting our hypothesis functions. Suppose we are using linear regression, but we do not know whether to select $$h_\theta(x) = \theta_0 + \theta_1 x$$, or $$h_\theta(x) = \theta_0 + \theta_1 x + \theta_2 x^2$$, or $$h_\theta(x) = \theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3$$, etc. The approach to this problem is simple. Try each of them and compute their error using another dataset. The one whose error is smallest should be selected.

So here, instead of partitioning our dataset into 2 parts, we partition it into 3 parts: training set (maybe 60%), cross validation set (maybe 20%), and testing set (the rest). First use training set to training each model, and then use cross validation set to compute the $$J_{cv}(\theta)$$ for them and make the model selection decision, and finally use the testing set to compute the $$J_{test}(\theta)$$. For the same reason why we cannot use training set for testing purposes, we can neither use the cross validation set for testing purposes.

## 10.4 Diagnosing Bias vs Variance
As we know, high bias is also called underfitting, and high variance is also called overfitting. Both of them are unacceptable properties for our hypothesis function. There has to be a way to diagnose them.

Empirically, when the degree of our polynomial grows (i.e. the features increase), our model will be more likely to fit our training set, but also to fail our cross validation. Meanwhile, when the degree of our polynomial shrinks (i.e. the features decrease), our model will be more likely to produce high error value in both training and cross validation. Below is the plot of training errors and cross validation errors:

![Errors Plot]({{ 'assets/img/post_img/2019-07-17-ErrorsPlot.png' | relative_url }})

According to this plot, we can see:
* To diagnose underfitting:
    * $$J_{train}(\theta)$$ will be high.
    * $$J_{train}(\theta) \approx J_{cv}(\theta)$$.
* To diagnose overfitting:
    * $$J_{train}(\theta)$$ will be low.
    * $$J_{train}(\theta) << J_{cv}(\theta)$$.

## 10.5 Regularization Affects to Bias and Variance
When doing regularization, we have to define the value of $$\lambda$$. We may start from 0.01, and slowly increases it to 0.02, 0.04, 0.08 etc. There has to be a way for us to find out the best $$\lambda$$. Actually, we can plot the training error function and the cross validation error function with the parameter $$\lambda$$. The plot is like below:

![Errors Plot with Lambda]({{ 'assets/img/post_img/2019-07-17-ErrorsPlotLambda.png' | relative_url }})

According to this plot, we can see: if training error is low but cross validation error is high, then it would be overfitting; if both training error and cross validation error are high, then it would be underfitting. We have to get a point somewhere in the middle.

## 10.6 Learning Curves
If we plot the training error function and cross validation error function with the dataset size $$m$$, we would have:

![Normal Learning Curve]({{ 'assets/img/post_img/2019-07-17-LearningCurveNormal.png' | relative_url }})

This is called the learning curves. As the dataset size increases, training error will increase more and more slowly, and cross validation error will decrease more and more slowly. What we care most is the cross validation error. In an underfitting model, the learning curves would be like the following:

![Bias Learning Curve]({{ 'assets/img/post_img/2019-07-17-LearningCurveBias.png' | relative_url }})

We can see that in a high-bias model, as the dataset size increases, the training error would be more and more close to the cross validation error, which is quite high. Adding more data will not help much.

And in an overfitting model, the learning curves would be like below:

![Variance Learning Curve]({{ 'assets/img/post_img/2019-07-17-LearningCurveVariance.png' | relative_url }})

The gap between training error and cross validation error would be quite huge. But with more data, the gap would get narrowed slowly. So adding more data is likely to help in a high-variace model.

