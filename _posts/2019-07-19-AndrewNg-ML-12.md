---
layout:	post
title:	"Machine Learning Notes: Lecture 12"
description: "Andrew Ng Machine Learning Notes for Lecture 12: Support Vector Machines"
date:	2019-07-19
tags:	["Machine Learning", "Andrew Ng"]
---

# 12. Support Vector Machines

## 12.1 Optimization Objective
Support Vector Machine is another method of supervised learning. It is derived from logistic regression.

Its hypothesis is way simpler than logistic regression:

$$ 
h_\theta(x) = \left\{\begin{array}{ll}
1, & if\ \theta^T x \geq 0\\
0, & otherwise
\end{array}
\right.
$$

And for its cost function, we have to introduce two more assistance functions:

$$
cost_0(z) = \left\{\begin{array}{ll}
0, & if\ z \leq -1 \\
1+z, & otherwise
\end{array}\right.
$$

$$
cost_1(z) = \left\{\begin{array}{ll}
0, & if\ z \geq 1 \\
1-z, & otherwise
\end{array}\right.
$$

And their plots compared to those of logistic regressions are like:

![SVM Cost-0]({{ 'assets/img/post_img/2019-07-19-SVM-Cost0.png' | relative_url }})
![SVM Cost-1]({{ 'assets/img/post_img/2019-07-19-SVM-Cost1.png' | relative_url }})

The final cost function is:

$$
J_\theta(x) = C [\sum_{i=1}^m y^{(i)} cost_1(\theta^T x^{(i)}) + (1-y^{(i)}) cost_0(\theta^T x^{(i)})] + \frac{1}{2} \sum_{j=1}^n \theta_j^2
$$

, where $$C$$ can be viewed as $$\frac{1}{\lambda}$$.

## 12.2 Large Margin Intuition
First consider the inner product of two vectors: $$v = \begin{bmatrix}v_1\\v_2\end{bmatrix}$$ and $$u = \begin{bmatrix}u_1\\u_2\end{bmatrix}$$. As we have already known, it is $$v_1 u_1 + v_2 u_2$$. Denote $$p$$ as the projection of $$v$$ on $$u$$, then $$p = ||v|| \cos< u,v > = \frac{u \cdot v}{||u||}$$. So $$u \cdot v = p ||u||$$.

![Inner Product]({{ 'assets/img/post_img/2019-07-20-InnerProduct.png' | relative_url }})

As a result, $$\theta^T x^{(i)} = \theta \cdot x^{(i)} = p^{(i)} \vert\vert \theta \vert\vert$$ where $$p^{(i)}$$ is the projection of $$x^{(i)}$$ on $$\theta$$. Also, it is obvious that $$\frac{1}{2} \sum_{j=1}^n \theta_j^2 = \frac{1}{2} \vert\vert \theta \vert\vert ^2$$. 

Let's suppose $$\theta_0 = 0$$ for convenience. From section 12.1 we have already known that to minimize the cost function, we have to either make $$\theta^T x^{(i)} \geq 1$$ or $$\theta^T x^{(i)} \leq -1$$. So it means that we have to make $$p^{(i)} \vert\vert \theta \vert\vert \geq 1$$ or $$p^{(i)} \vert\vert \theta \vert\vert \leq -1$$. Considering the regularization term $$\frac{1}{2} \vert\vert \theta \vert\vert ^2$$, we do not want $$\vert\vert \theta \vert\vert$$ to be too great (or too small). As a result, we hope $$p^{(i)}$$ to be great (or small). So the green line in the following picture would not be the hypothesis, because it makes the projection too close to zero.

![Wrong Hypothesis]({{ 'assets/img/post_img/2019-07-20-WrongHypothesis.png' | relative_url }})

And thus the green line in the next picture would be the hypothesis. Also, the smallest length of projection (should be symmatric for 2 classes) is called the *margin* of the hypothesis function. It can be viewed as the smallest distance between data nodes and the hypothesis plot.

![Wrong Hypothesis]({{ 'assets/img/post_img/2019-07-20-RightHypothesis.png' | relative_url }})

Also, when $$C$$ is not too large, i.e. $$\lambda$$ is not too small, it is more likely not to overfit. E.g. in the following picture, even if there are some strange data points, the hypothesis would stay the black line in the middle as long as $$C$$ is not too large. When $$C$$ is too large, the hypothesis would easily become the magenta line.

![Outliers]({{ 'assets/img/post_img/2019-07-20-Outliers.png' | relative_url }})

## 12.3 Kernels
Instead of using the input dataset $$x^{(i)}$$ directly, we can preprocess them to a feature value $$f_i$$ and use it in our hypothesis. First denote $$l^{(i)}$$ as *landmarks* whose value is equal to the training data. And we define

$$ f_i = similarity(x, l^{(i)}) = e^{-\frac{\vert\vert x - l^{(i)} \vert\vert ^2}{2\sigma^2}}$$

The gaussian function $$e^{-\frac{\vert\vert x - l^{(i)} \vert\vert ^2}{2\sigma^2}}$$ is called the *kernel function*. Kernel functions can be various, gaussian function is the common one (The "no kernel" is equivalent to "linear kernel").So our hypothesis would be: 

$$
h_\theta(x) = \left\{\begin{array}{ll}
1, & if\ \theta^T f \geq 0 \\
0, & otherwise
\end{array}\right.
$$

And the cost function would be:

$$ J(\theta) = C\sum_{i=1}^m y^{(i)} cost_1(\theta^T f^{(i)}) + (1-y^{(i)}) cost_0(\theta^T f^{(i)}) + \frac{1}{2} \sum_{j=1}^m \theta_j^2$$

It is easy to understand that, the feature $$f$$ stands for the similarity between the data and each landmark, and the vector $$\theta$$ controls which landmarks or similarities are of more importance. The more similar with a landmark $$l^{(i)}$$, the more close to 1 its $$f_i$$ will be, and along with the weight $$\theta_i$$, the greater $$\theta^T f$$ will be, so the smaller its cost will be.

In addition, when $$\sigma^2$$ is large, the feature $$f_i$$ will vary more smoothly, which means higher bias and lower variance. On the other hand, when $$\sigma^2$$ is small, the feature $$f_i$$ will vary less smoothly, which means higher variance and lower bias.

## 12.3 Practice

Besides the gaussian kernel, we have:
* Polynomial kernels: $$k(x,l) = (x^T l + constant)^{degree}$$
* String kernel
* Chi-square kernel
* Histogram intersection kernel

When it comes to multi-class problems, many SVM packages have built-in functionality. To do it manually, use one-vs-all method and select the highest $$\theta^T f$$.

When to use logistic regression or SVMs: denote $$n$$ as the number of features and $$m$$ as the size of training set. 
* If $$n$$ is much larger than $$m$$, use logistic regreesion or SVM without a kernel. 
* If $$n$$ is small and $$m$$ is intermediate, use SVM with gaussian kernel. 
* If $$n$$ is much smaller than $$m$$, try to add more features and use logistic regreesion or SVM without a kernel.
