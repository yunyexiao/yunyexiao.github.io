---
layout:	post
title:	"Machine Learning Notes: Lecture 16"
description: "Andrew Ng Machine Learning Notes for Lecture 16: Recommender System"
date:	2019-07-21
tags:	["Machine Learning", "Andrew Ng"]
---

# 16. Recommender System
Suppose we have a dataset about several users' ratings on several movies. Denote the number of people by $$n_u$$, and the number of movies by $$n_m$$. A user can rate a movie in between 0-5 or choose not to rate. Denote whether a user $$j$$ rated a movie $$i$$ by $$r(i,j)$$. Denote the rating given by user $$j$$ on movie $$i$$ by $$y^{(i,j)}$$.

## 16.1 Content-based Recommender
Suppose each movie has $$n$$ given features. Denote the feature values of movie $$i$$ by vector $$x^{(i)}$$. For each user $$j$$, we have to learn a parameter $$\theta^{(j)}$$ so that $$(\theta^{(j)})^T x^{(i)}$$ can predict the rating user $$j$$ may give movie $$i$$. This is a linear regression.

According to linear regression, ignore the $$m$$ in the denominator and the cost function for user $$j$$ would be

$$J(\theta^{(j)}) = \frac{1}{2} \sum_{i:r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{k=1}^n (\theta_k^{(j)})^2$$

And the cost function for all users would be:

$$J(\theta) = \frac{1}{2} \sum_{j=1}^{n_u} \sum_{i:r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n (\theta_k^{(j)})^2$$

So the gradient descent update rules:

$$\theta_k^{(j)} := \theta_k^{(j)} - \alpha \sum_{i:r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) x_k^{(i)}$$ when $$k = 0$$;

$$\theta_k^{(j)} := \theta_k^{(j)} (1 - \alpha \lambda) - \alpha \sum_{i:r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) x_k^{(i)}$$ when $$k \geq 1$$.

## 16.2 Collaborative Filtering Algorithm
From the section above we can see that given features $$x$$ of each movie we can learn $$\theta$$ for each user. Similarly, given the $$\theta$$ for each user we can also learn the features $$x$$ of each movie. So here is a simple consideration when we have neither features nor $$\theta$$: randomly initialize features and learn $$\theta$$, and then use the $$\theta$$ to update features $$x$$, and iterate these steps until coverage. This method is called *collaborative filtering algorithm*. It can work well, but is computationally expensive. So we have it simplified. The most common collaborative filtering algorithm does not have to run those iterations. It is just a gradient descent. The cost function is defined as follows:

$$J(x,\theta) = \frac{1}{2} \sum_{(i,j):r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n (\theta_k^{(j)})^2 + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^n (x_k^{(i)})^2$$

And its gradient descent update rules:

$$x_k^{(i)} := x_k^{(i)} (1 - \alpha \lambda) - \alpha \sum_{j:r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) \theta_k^{(j)}$$

$$\theta_k^{(j)} := \theta_k^{(j)} (1 - \alpha \lambda) - \alpha \sum_{i:r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) x_k^{(i)}$$

The hypothesis function remains $$(\theta^{(j)})^T x^{(i)}$$.

Define $$X = \begin{bmatrix}(x^{(1)})^T\\(x^{(2)})^T\\...\\(x^{(n_m)})^T\end{bmatrix}$$, and $$\Theta = \begin{bmatrix}\theta^{(1)} & \theta^{(2)} & ... & \theta^{(n_u)}\end{bmatrix}$$. The hypothesis function can be rewrite as $$h(X, \Theta) = X \Theta$$.

## 16.3 Mean Normalization
For the recommender above, there is a problem. If there is a user who did not rate any movie, then the his prediction rating would be zero to each movie. But 0 is the worst rating. It does not make sense to predict the worst rating if a user did not rate anything. In this case, mean values should be the prediction result.

So the idea of mean normalization is to compute vector $$\mu$$ which denotes the average rating of each movie, and substract each $$y^{(i,j)}$$ by the corresponding mean value. Also, the hypothesis function should be: $$h(X, \Theta) = X \Theta + \mu A$$ where $$A$$ is a horizontal vector filled with 1.
