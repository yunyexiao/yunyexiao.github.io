---
layout:	post
title:	"Machine Learning Notes: Lecture 14"
description: "Andrew Ng Machine Learning Notes for Lecture 14: Dimansionality Reduction"
date:	2019-07-21
tags:	["Machine Learning", "Andrew Ng"]
---

# 14. Dimensionality Redution

## 14.1 Motivation
Sometimes the dimension of our data is too huge (e.g. $$10^4$$) thus to add to the difficulty of computation. Data compression is one motivation of demensionality reduction. Data visualization is another motivation. We can plot the data up to only 3 dimensions. It is hard for us to plot a 4D dataset or even more dimensions. Thus reducing dimension to 1-3 is more understandable for us.

## 14.2 Principle Component Analysis (PCA)
The whole principle is somewhat similar to linear regression: find a proper space which minimizes the so-called projection errors and project all data points onto it. The projection error is the distance from a data point to the space you select in the original dimension. Here is the algorithm. Suppose we want to reduce the data from $$n$$-dimensions to $$k$$-dimensions ($$n > k$$).
1. Preprocess the input data. Use feature scaling and mean normalization (See *Machine Learning Notes: Lecture 3-4*).
2. Compute *Covariance Matrix*: $$\Sigma = \frac{1}{m} \sum_{i=1}^m (x^{(i)})(x^{(i)})^T$$.
3. Compute *Eigenvectors* of the covariance matrix: $$[U,S,V] = svd(\Sigma)$$. $$eig(\Sigma)$$ function also works. But $$svd$$ is more stable. In the return values, $$U$$ is the $$n \times n$$ matrix of our interest. The first $$k$$ columns of $$U$$ is the vectors defining the selected space. Denote $$U$$ by $$\begin{bmatrix}u^{(1)} & u^{(2)} & ... & u^{(n)}\end{bmatrix}$$ where each $$u^{(i)}$$ is an $$n$$-dimension vector, then $$U_{reduce} = \begin{bmatrix}u^{(1)} & u^{(2)} & ... & u^{(k)}\end{bmatrix}$$ is the reduced matrix.
4. Compute the projected data point: $$z^{(i)} = U_{reduce}^T x^{(i)}$$.

## 14.3 Choosing the Number of Principle Components ($$k$$)
Denote average squared projection error by $$\frac{1}{m} \sum_{i=1}^m \vert\vert x^{(i)} - x_{approx}^{(i)} \vert\vert ^2$$ where $$x_{approx}^{(i)}$$ means the projection of $$x^{(i)}$$ on the selected $$k$$-dimension space. Denote total variance in the data by $$\frac{1}{m} \sum_{i=1}^m \vert\vert x^{(i)} \vert\vert ^2$$. Our principle of choosing $$k$$ is to choose the smallest $$k$$ so that the ratio between the average squared projection error and the total variance can be no greater than 0.01, i.e.

$$ \frac{\sum_{i=1}^m \vert\vert x^{(i)} - x_{approx}^{(i)} \vert\vert ^2}{\sum_{i=1}^m \vert\vert x^{(i)} \vert\vert ^2} \leq 0.01$$

which means 99% of variance is retained. Other numbers (e.g. 0.05/95%) is also available.

Recall that in the section 14.2, we have $$[U,S,V] = svd(\Sigma)$$. We have used the matrix $$U$$ in that section, but other 2 matrices untouched. Here comes the matrix $$S$$. $$S$$ is a $$n \times n$$ diagonal matrix. The ratio between projection error and variance can be computed by $$1 - \frac{\sum_{i=1}^k S_{ii}}{\sum_{i=1}^n S_{ii}}$$. So the condition can be rewrite to:

$$\frac{\sum_{i=1}^k S_{ii}}{\sum_{i=1}^n S_{ii}} \geq 0.99$$

Actually, you only have to run the svd function once, and try each $$k$$ using the matrix $$S$$. You do not have to run the whole PCA algorithm from scratch over and over again.

One special case: if you use PCA for data visualization purposes, just try $$k=1,2,3$$.

## 14.4 Reconstruction of The Original Data
Just use the formula $$x_{approx}^{(i)} = U_{reduce} z^{(i)}$$. The result is just an approximation value of the original data point. It is easy to prove that $$x_{approx}$$ is not always equal to $$x$$.

## 14.5 Applying PCA
As PCA can reduce the number of features from $$n$$ to $$k$$, it may be misused to prevent overfitting problem. Actually, the projected data you get is somewhat distorted when compression. It is usually not a good choice to prevent overfitting.

Also, try run your machine learning code without PCA first. Only if the result is disappointing should you use PCA.
