---
layout:	post
title:	"Machine Learning Notes: Lecture 13"
description: "Andrew Ng Machine Learning Notes for Lecture 13: Clustering"
date:	2019-07-20
tags:	["Machine Learning", "Andrew Ng"]
---

# 13. Clustering
Category: unsupervised learning.

## 13.1 K-Means Algorithm
Denote the number of clusters by $$K$$ and the training set by $${x^{(1)}, x^{(2)},...,x^{(m)}}$$. The algorithm is iterative and each iteration can be splited into two steps: cluster assignment step and move-centroid step. The algorithm is like below:

1. Randomly initialize $$K$$ cluster centroids $$\mu_k,k=1,2,...,K$$.
2. Repeat the following steps until coverage:
    1. for i = 1 to m, assign $$c^{(i)}$$ by the index of cluster centroid which is closest to the point $$x^{(i)}$$.
    2. for k = 1 to K, update $$\mu_k$$ to the average of points assigned to cluster $$k$$.

In fact, the optimization objective of K-Means algorithm is to minimize the following cost function:

$$J(c, \mu) = \frac{1}{m} \sum_{i=1}^m \vert\vert x^{(i)} - \mu_{c^{(i)}}\vert\vert ^2$$

It is also called the *distortion function*. The cluster assignment step minimizes $$J(c, \mu)$$ with respect to $$c$$, and the move-centroid step minimizes $$J(c, \mu)$$ with respect to $$\mu$$.

## 13.2 Random Initialization
Often, we can randomly choose some points in the training set and set them as the initial centroids. But the cost function may have more than one local optima. When $$K$$ is somewhere in between 2 and 10, then we should also run k-means many times to try to get to the global optima. Here is the algorithm:

1. Run the following steps 50-1000 times:
    1. Randomly initialize k-means.
    2. Run k-means, and get $$c$$ and $$\mu$$.
    3. Compute its cost using the distortion function.
2. Choose the clustering that gives the lowest cost.

## 13.3 Choosing $$K$$
There are two ways to choose a proper $$K$$ for the k-means algorithm.

The first one is called *Elbow method*. You try different $$K$$ values, compute the corresponding costs, and plot them. If you can find a point called *elbow* which satisfies:
* before this elbow, the cost decreases rapidly
* after this elbow, the cost decreases slowly

![Elbow Method]({{ 'assets/img/post_img/2019-07-20-ElbowMethod.png' | relative_url }})

Then the elbow point is the right choice. If the plot turns out to be too smooth to find an elbow, try the next way.

The second way is to consider your later purpose on these clusters. If a certain $$K$$ produces clusters that can work well later, just choose it.
